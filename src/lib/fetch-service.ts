import { supabase } from './supabase'

export interface FetchResult {
  success: boolean
  newItems: number
  error?: string
  items?: Array<{
    url: string
    title: string
    author: string
    published_at: string
    content: string
    tags: string[]
    metadata: Record<string, unknown>
  }>
}

export interface GitHubRepoData {
  name: string
  description: string
  html_url: string
  updated_at: string
  stargazers_count: number
  forks_count: number
  language: string
  topics: string[]
}

export class FetchService {
  // ä» GitHub ä»“åº“è·å–æœ€æ–°ä¿¡æ¯
  static async fetchGitHubRepo(repoUrl: string): Promise<FetchResult> {
    try {
      // è§£æ GitHub URL
      const match = repoUrl.match(/github\.com\/([^\/]+)\/([^\/]+)/)
      if (!match) {
        throw new Error('Invalid GitHub repository URL')
      }
      
      const [, owner, repo] = match
      const apiUrl = `https://api.github.com/repos/${owner}/${repo}`
      
      const response = await fetch(apiUrl, {
        headers: {
          'Accept': 'application/vnd.github+json',
          'User-Agent': 'NewsIntelligence/1.0'
        }
      })
      
      if (!response.ok) {
        throw new Error(`GitHub API error: ${response.status}`)
      }
      
      const repoData: GitHubRepoData = await response.json()
      
      // è·å–æœ€è¿‘çš„ commits
      const commitsResponse = await fetch(`${apiUrl}/commits?per_page=10`, {
        headers: {
          'Accept': 'application/vnd.github+json',
          'User-Agent': 'NewsIntelligence/1.0'
        }
      })
      
      const commits = commitsResponse.ok ? await commitsResponse.json() as Array<{
        html_url: string
        sha: string
        commit: {
          message: string
          author: {
            name: string
            date: string
          }
        }
      }> : []
      
      // è·å–æœ€è¿‘çš„ releases
      const releasesResponse = await fetch(`${apiUrl}/releases?per_page=5`, {
        headers: {
          'Accept': 'application/vnd.github+json',
          'User-Agent': 'NewsIntelligence/1.0'
        }
      })
      
      const releases = releasesResponse.ok ? await releasesResponse.json() as Array<{
        html_url: string
        name: string
        tag_name: string
        body: string
        published_at: string
        author: {
          login: string
        }
      }> : []
      
      // æ„å»ºé¡¹ç›®æ•°æ®
      const items: Array<{
        url: string
        title: string
        author: string
        published_at: string
        content: string
        tags: string[]
        metadata: Record<string, unknown>
      }> = [
        {
          url: repoData.html_url,
          title: `Repository Update: ${repoData.name}`,
          author: owner,
          published_at: repoData.updated_at,
          content: repoData.description || 'No description available',
          tags: repoData.topics || [],
          metadata: {
            stars: repoData.stargazers_count,
            forks: repoData.forks_count,
            language: repoData.language,
            type: 'repository_info'
          }
        }
      ]
      
      // æ·»åŠ æœ€è¿‘çš„ commits
      commits.forEach((commit) => {
        items.push({
          url: commit.html_url,
          title: `Commit: ${commit.commit.message.split('\n')[0]}`,
          author: commit.commit.author.name,
          published_at: commit.commit.author.date,
          content: commit.commit.message,
          tags: ['commit'],
          metadata: {
            sha: commit.sha,
            type: 'commit'
          }
        })
      })
      
      // æ·»åŠ æœ€è¿‘çš„ releases
      releases.forEach((release) => {
        items.push({
          url: release.html_url,
          title: `Release: ${release.name || release.tag_name}`,
          author: release.author?.login || 'Unknown',
          published_at: release.published_at,
          content: release.body || 'No release notes',
          tags: ['release'],
          metadata: {
            tag_name: release.tag_name,
            type: 'release'
          }
        })
      })
      
      return {
        success: true,
        newItems: items.length,
        items
      }
      
    } catch (error) {
      console.error('GitHub fetch error:', error)
      return {
        success: false,
        newItems: 0,
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }
  
  // ä» RSS æºè·å–æ•°æ®
  static async fetchRSSFeed(rssUrl: string, sourceId?: string): Promise<FetchResult> {
    try {
      // ä½¿ç”¨ä»£ç†æœåŠ¡æ¥é¿å… CORS é—®é¢˜
      const proxyUrl = `https://api.allorigins.win/get?url=${encodeURIComponent(rssUrl)}`
      
      const response = await fetch(proxyUrl)
      if (!response.ok) {
        throw new Error(`Failed to fetch RSS: ${response.statusText}`)
      }
      
      const data = await response.json()
      let rssContent = data.contents
      
      // æ£€æŸ¥æ˜¯å¦æ˜¯ base64 ç¼–ç çš„å†…å®¹
      if (rssContent.startsWith('data:application/atom+xml; charset=utf-8;base64,')) {
        const base64Data = rssContent.split(',')[1]
        rssContent = Buffer.from(base64Data, 'base64').toString('utf-8')
      }
      
      // ç®€å•çš„ XML è§£æ
      const items: Array<{
        url: string
        title: string
        author: string
        published_at: string
        content: string
        tags: string[]
        metadata: Record<string, unknown>
      }> = []
      
      // è§£æ RSS/Atom feed
      const entryMatches = rssContent.match(/<entry[^>]*>[\s\S]*?<\/entry>/g) || 
                          rssContent.match(/<item[^>]*>[\s\S]*?<\/item>/g) || []
      
      for (const entry of entryMatches) {
        const titleMatch = entry.match(/<title[^>]*>([^<]*)<\/title>/i)
        const linkMatch = entry.match(/<link[^>]*href="([^"]*)"[^>]*>/i) || 
                         entry.match(/<link[^>]*>([^<]*)<\/link>/i)
        const authorMatch = entry.match(/<author[^>]*>[\s\S]*?<name[^>]*>([^<]*)<\/name>[\s\S]*?<\/author>/i) ||
                           entry.match(/<dc:creator[^>]*>([^<]*)<\/dc:creator>/i)
        const dateMatch = entry.match(/<updated[^>]*>([^<]*)<\/updated>/i) ||
                         entry.match(/<pubDate[^>]*>([^<]*)<\/pubDate>/i)
        const contentMatch = entry.match(/<content[^>]*>([^<]*)<\/content>/i) ||
                            entry.match(/<description[^>]*>([^<]*)<\/description>/i)
        
        if (titleMatch && linkMatch) {
          // è§£ç  HTML å®ä½“
          const decodeHtml = (html: string) => {
            return html
              .replace(/&lt;/g, '<')
              .replace(/&gt;/g, '>')
              .replace(/&amp;/g, '&')
              .replace(/&#39;/g, "'")
              .replace(/&quot;/g, '"')
              .replace(/<[^>]*>/g, '') // ç§»é™¤ HTML æ ‡ç­¾
              .trim()
          }
          
          const title = decodeHtml(titleMatch[1])
          const content = contentMatch ? decodeHtml(contentMatch[1]) : title
          
          // å¯¹äº GitHub commitsï¼Œå°è¯•è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
          let enhancedContent = content
          if (content === 'weekly updates' || content === title) {
            enhancedContent = `GitHub commit: ${title}\n\nThis appears to be a regular update commit. The actual changes may contain new papers or research updates.`
          }
          
          items.push({
            url: linkMatch[1],
            title: title,
            author: authorMatch ? authorMatch[1].trim() : 'Unknown',
            published_at: dateMatch ? new Date(dateMatch[1]).toISOString() : new Date().toISOString(),
            content: enhancedContent,
            tags: ['commit', 'github'],
            metadata: {
              type: 'rss_item',
              source: 'github_commits_rss',
              originalContent: contentMatch ? contentMatch[1] : null
            }
          })
        }
      }
      
      // ä¿å­˜åˆ°æ•°æ®åº“
      const { data: existingItems, error: fetchError } = await supabase
        .from('items')
        .select('url')
        .in('url', items.map(item => item.url))

      if (fetchError) throw fetchError

      const existingUrls = new Set(existingItems?.map(item => item.url))
      const newItems = items.filter(item => !existingUrls.has(item.url))

      if (newItems.length > 0) {
        const { error: insertError } = await supabase
          .from('items')
          .insert(newItems.map(item => ({
            source_id: sourceId || 'unknown-source',
            url: item.url,
            title: item.title,
            author: item.author,
            published_at: item.published_at,
            content: item.content,
            tags: item.tags,
            raw_ref: JSON.stringify(item.metadata),
            content_sha: item.url
          })))
        if (insertError) throw insertError
      }

      return {
        success: true,
        newItems: newItems.length,
        items: newItems
      }
    } catch (error) {
      console.error('RSS fetch error:', error)
      return {
        success: false,
        newItems: 0,
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }
  
  // ä» GitHub README è·å–æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
  static async fetchHTMLContent(htmlUrl: string, sourceId?: string): Promise<FetchResult> {
    try {
      console.log('Fetching GitHub README content from:', htmlUrl)
      
      // ä¸“é—¨å¤„ç† GitHub README
      if (htmlUrl.includes('github.com') && htmlUrl.includes('README.md')) {
        // æå–ä»“åº“ä¿¡æ¯
        const repoMatch = htmlUrl.match(/github\.com\/([^\/]+)\/([^\/]+)/)
        if (!repoMatch) {
          throw new Error('Invalid GitHub URL format')
        }
        
        const [, owner, repo] = repoMatch
        
        // ä½¿ç”¨ GitHub API è·å– README åŸå§‹å†…å®¹
        const githubApiUrl = `https://api.github.com/repos/${owner}/${repo}/readme`
        console.log('Fetching from GitHub API:', githubApiUrl)
        
        const response = await fetch(githubApiUrl, {
          headers: {
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'News-Intelligence-System'
          }
        })
        
        if (!response.ok) {
          throw new Error(`GitHub API error: ${response.status} ${response.statusText}`)
        }
        
        const data = await response.json()
        
        // è§£ç  base64 å†…å®¹
        const readmeContent = Buffer.from(data.content, 'base64').toString('utf-8')
        console.log('README content length:', readmeContent.length)
        
        // ç«‹å³è¿›è¡Œ AI åˆ†æï¼Œç”Ÿæˆæ›´æ–°æ€»ç»“
        let aiSummary = ''
        try {
          console.log('Generating AI summary for README...')
          const { AIService } = await import('./ai-service')
          
          const analysisResult = await AIService.analyzeContent([{
            title: `${owner}/${repo} - README æ›´æ–°`,
            content: readmeContent,
            url: htmlUrl,
            published_at: new Date().toISOString(),
            author: owner
          }], `${owner}/${repo}`, 'è¿™æ˜¯ä¸€ä¸ªè›‹ç™½è´¨è®¾è®¡è®ºæ–‡é›†åˆçš„ GitHub READMEï¼Œè¯·é‡ç‚¹å…³æ³¨æœ€æ–°æ·»åŠ çš„è®ºæ–‡ã€ç ”ç©¶æ–¹æ³•ã€æŠ€æœ¯çªç ´ã€ä½œè€…ä¿¡æ¯ç­‰')
          
          // ç”Ÿæˆç®€æ´çš„æ›´æ–°æ€»ç»“
          aiSummary = `ğŸ“š **${owner}/${repo} æœ€æ–°æ›´æ–°**\n\n`
          
          if (analysisResult.summary) {
            aiSummary += `**æ›´æ–°æ¦‚è§ˆï¼š**\n${analysisResult.summary}\n\n`
          }
          
          if (analysisResult.insights && analysisResult.insights.length > 0) {
            aiSummary += `**é‡è¦å‘ç°ï¼š**\n`
            analysisResult.insights.slice(0, 3).forEach((insight: string, index: number) => {
              aiSummary += `${index + 1}. ${insight}\n`
            })
            aiSummary += '\n'
          }
          
          if (analysisResult.keyEntities && analysisResult.keyEntities.length > 0) {
            aiSummary += `**å…³é”®è®ºæ–‡ï¼š**\n`
            analysisResult.keyEntities.slice(0, 5).forEach((entity: string) => {
              aiSummary += `â€¢ ${entity}\n`
            })
          }
          
          console.log('AI summary generated successfully')
        } catch (error) {
          console.error('AI analysis failed, using fallback:', error)
          // å¦‚æœ AI åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬æå–
          const lines = readmeContent.split('\n')
          const latestPapersLine = lines.find(line => line.includes('Papers last week'))
          if (latestPapersLine) {
            aiSummary = `ğŸ“š **${owner}/${repo} æœ€æ–°æ›´æ–°**\n\n${latestPapersLine}\n\næ£€æµ‹åˆ°æœ€æ–°è®ºæ–‡æ›´æ–°ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æ‘˜è¦è·å–è¯¦ç»†ä¿¡æ¯ã€‚`
          } else {
            aiSummary = `ğŸ“š **${owner}/${repo} README æ›´æ–°**\n\næ£€æµ‹åˆ° README å†…å®¹æ›´æ–°ï¼ŒåŒ…å«è›‹ç™½è´¨è®¾è®¡ç›¸å…³çš„æœ€æ–°è®ºæ–‡å’Œç ”ç©¶è¿›å±•ã€‚`
          }
        }
        
        // åˆ›å»ºå•ä¸ªæ¡ç›®ï¼ŒåŒ…å« AI åˆ†æåçš„å†…å®¹
        const items: Array<{
          url: string
          title: string
          author: string
          published_at: string
          content: string
          tags: string[]
          metadata: Record<string, unknown>
        }> = []
        
        items.push({
          url: htmlUrl,
          title: `${owner}/${repo} - æœ€æ–°æ›´æ–°`,
          author: owner,
          published_at: new Date().toISOString(),
          content: aiSummary,
          tags: ['github', 'readme', 'protein-design', 'deep-learning', 'ai-summary'],
          metadata: {
            type: 'github_readme_ai_summary',
            source: 'github_api',
            owner: owner,
            repo: repo,
            sha: data.sha,
            size: data.size,
            originalUrl: htmlUrl,
            originalContent: readmeContent.substring(0, 500) // ä¿å­˜éƒ¨åˆ†åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
          }
        })
        
        // ä¿å­˜åˆ°æ•°æ®åº“
        const { data: existingItems, error: fetchError } = await supabase
          .from('items')
          .select('url')
          .in('url', items.map(item => item.url))

        if (fetchError) throw fetchError

        const existingUrls = new Set(existingItems?.map(item => item.url))
        const newItems = items.filter(item => !existingUrls.has(item.url))

        if (newItems.length > 0) {
          const { error: insertError } = await supabase
            .from('items')
            .insert(newItems.map(item => ({
              source_id: sourceId || 'unknown-source',
              url: item.url,
              title: item.title,
              author: item.author,
              published_at: item.published_at,
              content: item.content,
              tags: item.tags,
              raw_ref: JSON.stringify(item.metadata),
              content_sha: item.url
            })))
          if (insertError) throw insertError
        }

        return {
          success: true,
          newItems: newItems.length,
          items: newItems
        }
      } else {
        // å¤„ç†å…¶ä»– HTML å†…å®¹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        const proxyUrl = `https://api.allorigins.win/get?url=${encodeURIComponent(htmlUrl)}`
        
        const response = await fetch(proxyUrl)
        if (!response.ok) {
          throw new Error(`Failed to fetch HTML: ${response.statusText}`)
        }
        
        const data = await response.json()
        let htmlContent = data.contents
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯ base64 ç¼–ç çš„å†…å®¹
        if (htmlContent.startsWith('data:text/html; charset=utf-8;base64,')) {
          const base64Data = htmlContent.split(',')[1]
          htmlContent = Buffer.from(base64Data, 'base64').toString('utf-8')
        }
        
        const titleMatch = htmlContent.match(/<title[^>]*>([^<]*)<\/title>/i)
        const contentMatch = htmlContent.match(/<body[^>]*>([\s\S]*?)<\/body>/i)
        
        const items = [{
          url: htmlUrl,
          title: titleMatch ? titleMatch[1].trim() : 'HTML Content',
          author: 'Unknown',
          published_at: new Date().toISOString(),
          content: contentMatch ? contentMatch[1].replace(/<[^>]*>/g, ' ').trim().substring(0, 1000) : 'HTML content',
          tags: ['html'],
          metadata: {
            type: 'html_content',
            source: 'html_fetch',
            originalUrl: htmlUrl
          }
        }]
        
        // ä¿å­˜åˆ°æ•°æ®åº“
        const { data: existingItems, error: fetchError } = await supabase
          .from('items')
          .select('url')
          .in('url', items.map(item => item.url))

        if (fetchError) throw fetchError

        const existingUrls = new Set(existingItems?.map(item => item.url))
        const newItems = items.filter(item => !existingUrls.has(item.url))

        if (newItems.length > 0) {
          const { error: insertError } = await supabase
            .from('items')
            .insert(newItems.map(item => ({
              source_id: sourceId || 'unknown-source',
              url: item.url,
              title: item.title,
              author: item.author,
              published_at: item.published_at,
              content: item.content,
              tags: item.tags,
              raw_ref: JSON.stringify(item.metadata),
              content_sha: item.url
            })))
          if (insertError) throw insertError
        }

        return {
          success: true,
          newItems: newItems.length,
          items: newItems
        }
      }
      
    } catch (error) {
      console.error('GitHub README fetch error:', error)
      return {
        success: false,
        newItems: 0,
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }
  
  // æ‰§è¡Œæ•°æ®æŠ“å–å¹¶ä¿å­˜åˆ°æ•°æ®åº“
  static async fetchAndSave(sourceId: string): Promise<FetchResult> {
    try {
      // è·å–æºä¿¡æ¯
      const { data: source, error: sourceError } = await supabase
        .from('sources')
        .select('*')
        .eq('id', sourceId)
        .single()
      
      if (sourceError || !source) {
        throw new Error('Source not found')
      }
      
      // è®°å½•æŠ“å–å¼€å§‹
      const { data: fetchRun } = await supabase
        .from('fetch_runs')
        .insert({
          source_id: sourceId,
          started_at: new Date().toISOString(),
          ok: false
        })
        .select()
        .single()
      
      let fetchResult: FetchResult
      
      // æ ¹æ®æºç±»å‹é€‰æ‹©æŠ“å–æ–¹æ³•
      switch (source.kind) {
        case 'github_repo':
          fetchResult = await this.fetchGitHubRepo(source.handle)
          break
        case 'rss':
          fetchResult = await this.fetchRSSFeed(source.handle, sourceId)
          break
        case 'html':
          fetchResult = await this.fetchHTMLContent(source.handle, sourceId)
          break
        default:
          throw new Error(`Unsupported source type: ${source.kind}`)
      }
      
      if (!fetchResult.success) {
        // æ›´æ–°æŠ“å–è®°å½•ä¸ºå¤±è´¥
        await supabase
          .from('fetch_runs')
          .update({
            ended_at: new Date().toISOString(),
            ok: false,
            error: fetchResult.error
          })
          .eq('id', fetchRun.id)
        
        return fetchResult
      }
      
      // ä¿å­˜æŠ“å–åˆ°çš„é¡¹ç›®åˆ°æ•°æ®åº“
      let savedCount = 0
      if (fetchResult.items) {
        for (const item of fetchResult.items) {
          // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäº URLï¼‰
          const { data: existing } = await supabase
            .from('items')
            .select('id')
            .eq('source_id', sourceId)
            .eq('url', item.url)
            .single()
          
          if (!existing) {
            // è®¡ç®—å†…å®¹å“ˆå¸Œç”¨äºå»é‡
            const contentSha = await this.calculateSHA(item.url + item.title + item.content)
            
            const { error: insertError } = await supabase
              .from('items')
              .insert({
                source_id: sourceId,
                url: item.url,
                title: item.title,
                author: item.author,
                published_at: item.published_at,
                content: item.content,
                content_sha: contentSha,
                tags: item.tags || []
              })
            
            if (!insertError) {
              savedCount++
            }
          }
        }
      }
      
      // æ›´æ–°æŠ“å–è®°å½•ä¸ºæˆåŠŸ
      await supabase
        .from('fetch_runs')
        .update({
          ended_at: new Date().toISOString(),
          ok: true,
          new_items: savedCount
        })
        .eq('id', fetchRun.id)
      
      return {
        success: true,
        newItems: savedCount
      }
      
    } catch (error) {
      console.error('Fetch and save error:', error)
      return {
        success: false,
        newItems: 0,
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }
  
  // è®¡ç®— SHA256 å“ˆå¸Œ
  private static async calculateSHA(text: string): Promise<string> {
    const encoder = new TextEncoder()
    const data = encoder.encode(text)
    const hashBuffer = await crypto.subtle.digest('SHA-256', data)
    const hashArray = Array.from(new Uint8Array(hashBuffer))
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('')
  }
}
